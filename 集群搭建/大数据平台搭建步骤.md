### Linux环境准备

#### 1. 配置静态IP（虚拟机）

#### 2. 修改主机名（虚拟机）

#### 3. 关闭防火墙

#### 4. 创建用户

#### 5. 配置该用户具有sudo root权限

#### 6. 配置免密登录

### Hadoop 安装

版本：hadoop 2.7.6

#### 1. 安装Java

#### 2. 关闭防火墙

```shell
# 查看防火墙状态
systemctl status firewalld.service

# 关闭防火墙
systemctl stop firewalld.service

# 关闭开机启动
systemctl disable firewalld.service
systemctl is-enabled firewalld.service

# 修改配置文件
vim /etc/selinux/config
SELINUX=disabled
```

#### 3. 同步时间ntp

```shell
yum -y install ntp
systemctl start ntpd.service
systemctl status ntpd.service
systemctl enable ntpd.service
```

#### 4. 配置host

```shell
192.168.0.126   master
192.168.0.127   slave1
192.168.0.128   slave2
192.168.0.129   slave3
192.168.0.130   slave4
```

#### 5. 配置免密登录

```shell
ssh-keygen -t rsa

# 其他5台机器
ssh-copy-id master
ssh-copy-id slave1
ssh-copy-id slave3
ssh-copy-id slave3
ssh-copy-id slave4
```

#### 6. 解压hadoop

```shell
tar -zxvf /data/soft/install_package/hadoop-2.7.6.tar.gz
```

#### 7. 配置hadoop

- core-site.xml

  ```xml
  <!-- 指定HDFS中NameNode的地址 -->
  <property>
      <name>fs.defaultFS</name>
      <value>hdfs://master:9000</value>
  </property>
  
  <!-- 指定Hadoop运行时产生文件的存储目录 -->
  <property>
      <name>hadoop.tmp.dir</name>
      <value>/data/soft/hadoop/data/tmp</value>
  </property>
  ```

- hadoop-env.sh

  ```sh
  export JAVA_HOME=/data/soft/java/jdk1.8.0_121
  ```

- hdfs-site.xml

  ```xml
  <!-- Hadoop副本数量 -->
  <property>
      <name>dfs.replication</name>
      <value>1</value>
  </property>
  
  <!-- 指定Hadoop辅助名称节点主机配置 -->
  <property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>slave2:50090</value>
  </property>
  ```

- yarn-env.sh

  ```shell
  export JAVA_HOME=/data/soft/java/jdk1.8.0_121
  ```

- yarn-site.xml

  ```xml
  <!-- Reducer获取数据的方式 -->
  <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
  </property>
  
  <!-- 指定YARN的ResourceManager的地址 -->
  <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>slave1</value>
  </property>
  ```

- mapred-env.sh

  ```shell
  export JAVA_HOME=/data/soft/java/jdk1.8.0_121
  ```

- mapred-site.xml

  ```xml
  <!-- 指定MR运行在Yarn上 -->
  <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
  </property>
  ```

- slaves

  ```shell
  vim /data/soft/hadoop/etc/hadoop/slaves
  
  # 添加节点
  master
  slave1
  slave2
  slave3
  slave4
  ```

#### 8. 分发安装包

```shell
scp -r /data/soft/hadoop root@slave1:/data/soft
scp -r /data/soft/hadoop root@slave2:/data/soft
scp -r /data/soft/hadoop root@slave3:/data/soft
scp -r /data/soft/hadoop root@slave4:/data/soft
```

#### 9. 启动hadoop集群

```shell
# 配置/etc/profile
# HADOOP_HOME
export HADOOP_HOME=/data/soft/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

# 在Namenode节点启动hdfs
start-dfs.sh
# 在yarn节点启动yarn
start-yarn.sh
```

#### 10. Hadoop几个常用UI界面

- HDFS监控页面：http://192.168.0.126:50070/
- MapReduce日志监控页面：http://192.168.0.126:19888/jobhistory
- Yarn集群信息页面：http://192.168.0.127:8088/cluster

### Hive 安装

#### 1. 解压hive

```shell
tar -zxvf apache-hive-2.3.7-bin.tar.gz
mv apache-hive-2.3.7-bin hive
```

#### 2. 安装mysql5.7

```shell
# 卸载Linux自带的mysql
rpm -qa|grep mysql
rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64
# 解压mysql安装包
tar xvf mysql-5.7.32-1.el7.x86_64.rpm-bundle.tar
# 依次安装服务
rpm -ivh mysql-community-common-5.7.32-1.el7.x86_64.rpm
rpm -ivh mysql-community-libs-5.7.32-1.el7.x86_64.rpm
rpm -ivh mysql-community-client-5.7.32-1.el7.x86_64.rpm
rpm -ivh mysql-community-server-5.7.32-1.el7.x86_64.rpm
# 数据库初始化
mysqld --initialize --user=mysql
# 查看root账号密码
cat /var/log/mysqld.log
# 启动mysql
systemctl start mysqld
# 设置密码
ALTER USER 'root'@'localhost' IDENTIFIED BY 'allchips';
# 配置远程访问
update user set host='%' where user='root' and host='localhost';
flush privileges;
# 远程Navicat连接验证
```

#### 3. Hive元数据配置到mysql

- 解压 mysql-connector-java-8.0.22.tar.gz

  ```shell
  tar -zxvf mysql-connector-java-8.0.22.tar.gz
  ```

- 拷贝驱动包

  ```shell
  cp mysql-connector-java-8.0.22.jar /data/soft/hive/lib/
  ```

- hive-site.xml

  ```xml
  <?xml version="1.0" encoding="UTF-8" standalone="no"?>
  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
  <configuration>
    <!-- Hive元数据配置到MySQL -->
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://slave3:3306/metastore?createDatabaseIfNotExist=true</value>
      <description>JDBC connect string for a JDBC metastore</description>
    </property>
  
    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.cj.jdbc.Driver</value>
      <description>Driver class name for a JDBC metastore</description>
    </property>
  
    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
      <description>username to use against metastore database</description>
    </property>
  
    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>allchips</value>
      <description>password to use against metastore database</description>
    </property>
      
    <property>
      <name>hive.metastore.schema.verification</name>
      <value>false</value>
    </property>
    <property>
      <name>datanucleus.schema.autoCreateAll</name>
      <value>true</value>
    </property>
    
    <!-- Hive default数据仓库原始位置 -->
    <property>
      <name>hive.metastore.warehouse.dir</name>
      <value>/user/hive/warehouse</value>
      <description>location of default database for the warehouse</description>
    </property>
    <property>
      <name>hive.exec.scratchdir</name>
      <value>/tmp/hive</value>
    </property>
    
    <!-- 配置hiveserver2 -->
    <property>
      <name>hive.server2.webui.host</name>
      <value>192.168.0.129</value>
    </property>
    <property>
      <name>hive.server2.webui.port</name>
      <value>10002</value>
    </property>
  </configuration>
  ```

- 初始化mysql

  ```shell
  schematool -initSchema -dbType mysql
  ```

#### 4. 配置hive

- hive-env.sh

  ```shell
  # 配置HADOOP_HOME路径
  export HADOOP_HOME=/data/soft/hadoop
  
  # 配置HIVE_CONF_DIR路径
  export HIVE_CONF_DIR=/data/soft/hive/conf
  export HIVE_AUX_JARS_PATH=/data/soft/hive/lib
  ```

- vim /etc/profile

  ```shell
  # 配置HIVE_HOME
  export HIVE_HOME=/data/soft/hive
  export PATH=$PATH:$HIVE_HOME/bin
  
  source /etc/profile
  ```

#### 5. 启动hive

```shell
# 在hdfs上新建目录
hadoop fs -mkdir -p /user/hive/warehouse
hadoop fs -chmod 777 /user/hive/warehouse
hadoop fs -mkdir -p /tmp/hive
hadoop fs -chmod 777 /tmp/hive

# 启动元数据服务
nohup hive --service metastore > metastore.log 2>&1 &
# 关闭后台任务
kill -9 pid

# 启动hiveserver2
nohup hive --service hiveserver2 > hiveserver2 .log 2>&1 &
# 关闭后台任务
kill -9 pid

# 查看hive进程
ps -aux|grep hive

# 启动hive Cli
hive
```

#### 6. 安装hive client

- 拷贝到集群中另外一个节点

  ```shell
  scp -r hive root@slave4:/data/soft/
  ```

- hive-site.xml

  ```xml
  <?xml version="1.0" encoding="UTF-8" standalone="no"?>
  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
  <configuration>
    <property>
      <name>hive.metastore.warehouse.dir</name>
      <value>/user/hive/warehouse</value>
    </property>
  
    <property>
      <name>hive.metastore.local</name>
      <value>false</value>
    </property>
  
    <property>
      <name>hive.metastore.uris</name>
      <value>thrift://slave3:9083</value>
    </property>
  </configuration>
  ```

- vim /etc/profile

  ```shell
  # 配置HIVE_HOME
  export HIVE_HOME=/data/soft/hive
  export PATH=$PATH:$HIVE_HOME/bin
  
  source /etc/profile
  ```

- 启动 metastore

  ```shell
  nohup hive --service metastore > metastore.log 2>&1 &
  
  # 启动hive
  hive
  ```

#### 7. 验证

```mysql
# 建表
use dp_test;
create table test(
id int,
name string
)
row format delimited fields terminated by '\t';

# 准备数据
1001	zhangshan
1002	lishi
1003	zhaoliu

# 加载数据
load data local inpath '/data/tmp/test.txt' into table test;

# 查询数据
select id,name from test;
```

#### 8. 其他

- 修改hive支持中文注释

  ```mysql
  # vim hive-site.xml
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://slave3:3306/metastore?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
    <description>JDBC connect string for a JDBC metastore</description>
  </property>
  
  # 进行数据库metastore执行
  alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;
  alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
  alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
  alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;
  
  # 重启metastore服务
  ps -aux|grep hive
  kill -9 pid
  nohup hive --service metastore > metastore.log 2>&1 &
  ```
  
- 配置hiveserver2

  ```xml
    <!-- 配置hiveserver2 -->
    <property>
      <name>hive.server2.webui.host</name>
      <value>192.168.0.129</value>
    </property>
    <property>
      <name>hive.server2.webui.port</name>
      <value>10002</value>
    </property>
      <property>
      <name>hive.server2.thrift.port</name>
      <value>10000</value>
    </property>
    <property>
      <name>hive.server2.thrift.bind.host</name>
      <value>192.168.0.129</value>
    </property>
    <property>
      <name>beeline.hs2.connection.user</name>
      <value>hive</value>
    </property>
    <property>
      <name>beeline.hs2.connection.password</name>
      <value>hive</value>
    </property>
  ```

### Spark 安装

#### 1. 解压spark

```shell
tar -zxvf spark-2.4.2-bin-hadoop2.7.tgz
mv spark-2.4.2-bin-hadoop2.7 spark
```

#### 2. 配置Spark

- spark-env.sh

  ```shell
  # 指定JAVA_HOME
  JAVA_HOME=/data/soft/java/jdk1.8.0_121
  #指定默认master的ip或主机名
  export SPARK_MASTER_HOST=slave5  
  #指定maaster提交任务的默认端口为7077    
  export SPARK_MASTER_PORT=7077 
  #指定masster节点的webui端口       
  export SPARK_MASTER_WEBUI_PORT=8080 
  #每个worker从节点能够支配的内存数 
  export SPARK_WORKER_MEMORY=2g        
  #允许Spark应用程序在计算机上使用的核心总数（默认值：所有可用核心）
  export SPARK_WORKER_CORES=2    
  #每个worker从节点的实例（可选配置） 
  export SPARK_WORKER_INSTANCES=2   
  #指向包含Hadoop集群的（客户端）配置文件的目录，运行在Yarn上配置此项   
  export HADOOP_CONF_DIR=/data/soft/hadoop/etc/hadoop
  ```

- slaves

  ```shell
  master
  slave1
  slave2
  slave3
  slave4
  ```

- vim /etc/profile

  ```shell
  # 配置SPARK_HOME
  export SPARK_HOME=/data/soft/spark
  export PATH=$PATH:$SPARK_HOME/bin
  export PATH=$PATH:$SPARK_HOME/sbin
  
  source /etc/profile
  ```

- 分发 spark

  ```shell
  scp -r /data/soft/spark root@master:/data/soft
  scp -r /data/soft/spark root@slave1:/data/soft
  scp -r /data/soft/spark root@slave2:/data/soft
  scp -r /data/soft/spark root@slave3:/data/soft
  ```

#### 3. 启动集群

```shell
# 启动master
start-master.sh
# 启动所有的slave
start-slaves.sh
# 关闭所有的slave
stop-slaves.sh
# 关闭master
stop-master.sh
```

#### 4. Spark UI界面

```shell
http://192.168.0.130:8080/
```

### Sqoop 安装

#### 1. 解压sqoop

```shell
tar -zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
mv sqoop-1.4.7.bin__hadoop-2.6.0 sqoop
```

#### 2. 配置sqoop

- sqoop-env.sh

  ```shell
  mv sqoop-env-template.sh sqoop-env.sh
  
  # 添加配置
  export JAVA_HOME=/data/soft/java/jdk1.8.0_121
  export HADOOP_COMMON_HOME=/data/soft/hadoop
  export HADOOP_MAPRED_HOME=/data/soft/hadoop
  export HIVE_HOME=/data/soft/hive
  ```

- vim /etc/profile

  ```shell
  # SQOOP_HOME
  export SQOOP_HOME=/data/soft/sqoop
  export PATH=$PATH:$SQOOP_HOME/bin
  ```

#### 3. 添加mysql驱动

#### 4. 校验sqoop

```shell
sqoop-version
```

#### 5. Sqoop导数太慢

```xml
<!-- 实测没啥用 -->
<property>
  <name>yarn.nodemanager.resource.memory-mb</name>
  <value>20480</value>
</property>
<property>
  <name>yarn.scheduler.minimum-allocation-mb</name>
  <value>2048</value>
</property>
<property>
  <name>yarn.nodemanager.vmem-pmem-ratio</name>
  <value>2.1</value>
</property>
```

### Hadoop 集群故障

- 解决方法

  ```shell
  问题：datanode启动失败，hadoop界面只有两个节点
  
  本次hadoop故障问题分享：
  1、从130同步所有etc到所有slave节点
  2、resource管理配置调整到master节点
  3、日志报创建数据失败，Initialization failed for Block pool时，重建127节点数据目录
  4、hadoop主从设置互信时，如果有别名（如master和slave），别名和ip都需要先连一次去掉互信确认
  即 ssh allchips@192.168.0.126 和 ssh allchips@master，其他机器同样
  ```